# vecbox - TODO
 
## ğŸ¯ **Objetivo Principal: Implementar embeddings locais com llama.cpp**
 
### **Contexto:**
- âœ… **Sharp removido** - DependÃªncia problemÃ¡tica eliminada
- âœ… **ONNX Runtime removido** - NÃ£o ideal para embeddings de linguagem
- âœ… **llama.cpp compilado** - Modelo nomic-embed-text-v1.5.Q4_K_M.gguf baixado
- ğŸ¯ **Meta:** Usar API nativa do llama.cpp sem dependÃªncias externas
 
---
 
## ğŸ“‹ **Tasks Divididas - ImplementaÃ§Ã£o llama.cpp**
 
### **Task 1: AnÃ¡lise da API llama.cpp**
- [ ] **Estudar estrutura do embedding.cpp**
  - [ ] Entender parÃ¢metros de linha de comando
  - [ ] Identificar formato de saÃ­da (JSON, array, raw)
  - [ ] Mapear opÃ§Ãµes de pooling e normalizaÃ§Ã£o
- [ ] **Analisar exemplos de uso**
  - [ ] Comando bÃ¡sico: `./llama-embedding -m model.gguf -p "texto"`
  - [ ] Batch processing: `--embd-separator` e `--embd-output-format`
  - [ ] OpÃ§Ãµes de GPU: `--n-gpu-layers`
 
### **Task 2: Arquitetura do Provider llama.cpp**
- [ ] **Criar LlamaCppProvider**
  - [ ] Herdar de EmbeddingProvider
  - [ ] Implementar detecÃ§Ã£o do llama.cpp na raiz do usuÃ¡rio
  - [ ] Configurar caminho do modelo GGUF
- [ ] **Implementar interface de comando**
  - [ ] Usar `child_process.spawn` para chamar llama-embedding
  - [ ] Capturar stdout/stderr para processamento
  - [ ] Parsear saÃ­da JSON/array para embedding
 
### **Task 3: DetecÃ§Ã£o e ConfiguraÃ§Ã£o**
- [ ] **Implementar detecÃ§Ã£o automÃ¡tica**
  - [ ] Buscar `./llama-embedding` ou `./build/bin/llama-embedding`
  - [ ] Verificar permissÃµes de execuÃ§Ã£o
  - [ ] Validar existÃªncia do modelo GGUF
- [ ] **ConfiguraÃ§Ã£o de caminhos**
  - [ ] Suporte a caminhos relativos e absolutos
  - [ ] Fallback para `~/llama.cpp/llama-embedding`
  - [ ] ConfiguraÃ§Ã£o via environment variables
 
### **Task 4: Processamento de Embeddings**
- [ ] **Processamento individual**
  - [ ] Executar comando com texto Ãºnico
  - [ ] Parsear saÃ­da para array de nÃºmeros
  - [ ] Aplicar normalizaÃ§Ã£o se necessÃ¡rio
- [ ] **Processamento em batch**
  - [ ] Usar `--embd-separator` para mÃºltiplos textos
  - [ ] Processar saÃ­da JSON para arrays
  - [ ] Otimizar performance para batches
 
### **Task 5: IntegraÃ§Ã£o com Factory**
- [ ] **Registrar LlamaCppProvider**
  - [ ] Adicionar ao EmbeddingFactory
  - [ ] Incluir no tipo ProviderType
  - [ ] Configurar como primeira opÃ§Ã£o no autoEmbed
- [ ] **Testes de integraÃ§Ã£o**
  - [ ] Testar com modelo nomic-embed-text-v1.5
  - [ ] Validar dimensÃµes (768 para nomic-embed-text-v1.5)
  - [ ] Testar fallback para providers de API
 
### **Task 6: Tratamento de Erros**
- [ ] **ValidaÃ§Ã£o de dependÃªncias**
  - [ ] Verificar se llama.cpp existe
  - [ ] Validar modelo GGUF disponÃ­vel
  - [ ] Mensagens de erro amigÃ¡veis
- [ ] **Fallback robusto**
  - [ ] Tentar providers de API se llama.cpp falhar
  - [ ] Logging detalhado para debug
  - [ ] Timeout e retry logic
 
### **Task 7: Performance e OtimizaÃ§Ã£o**
- [ ] **Cache de embeddings**
  - [ ] Cache em memÃ³ria para textos repetidos
  - [ ] PersistÃªncia opcional em disco
  - [ ] TTL para cache expiraÃ§Ã£o
- [ ] **OtimizaÃ§Ãµes**
  - [ ] Reutilizar processo llama.cpp se possÃ­vel
  - [ ] Streaming para textos longos
  - [ ] Batch processing automÃ¡tico
 
### **Task 8: DocumentaÃ§Ã£o e Exemplos**
- [ ] **DocumentaÃ§Ã£o de uso**
  - [ ] Como instalar e configurar llama.cpp
  - [ ] Exemplos de configuraÃ§Ã£o
  - [ ] Guia de troubleshooting
- [ ] **Exemplos prÃ¡ticos**
  - [ ] Uso bÃ¡sico com texto
  - [ ] Processamento de arquivos
  - [ ] Batch processing
 
---
 
## ğŸ¯ **Status Atual**
 
### **âœ… ConcluÃ­do:**
- âœ… AnÃ¡lise do problema sharp
- âœ… RemoÃ§Ã£o de dependÃªncias problemÃ¡ticas
- âœ… CompilaÃ§Ã£o do llama.cpp
- âœ… Download do modelo nomic-embed-text-v1.5.Q4_K_M.gguf
- âœ… AnÃ¡lise inicial da API llama.cpp
 
### **ğŸ”„ Em Progresso:**
- ğŸ”„ Estudo da API embedding.cpp
- ğŸ”„ Planejamento da arquitetura
 
### **â³ PrÃ³ximos Passos:**
- â³ Implementar LlamaCppProvider bÃ¡sico
- â³ Testar comando llama-embedding
- â³ Integrar com factory existente
 
---
 
## ğŸ“ **Notas Importantes:**
 
### **Design Decisions:**
1. **Sem dependÃªncias externas** - Usa llama.cpp nativo
2. **DetecÃ§Ã£o automÃ¡tica** - Busca na raiz do usuÃ¡rio
3. **Fallback inteligente** - API providers se local falhar
4. **Performance first** - Cache e otimizaÃ§Ãµes
5. **Minimalista** - Interface simples como design principle
 
### **Technical Considerations:**
- **Modelo alvo:** nomic-embed-text-v1.5 (768 dimensÃµes)
- **Formato:** GGUF quantizado (Q4_K_M)
- **SaÃ­da:** JSON ou array format
- **Pooling:** mean (padrÃ£o para embeddings)
- **NormalizaÃ§Ã£o:** euclidean (padrÃ£o)
 
### **Path Strategy:**
```
~/
â”œâ”€â”€ llama.cpp/
â”‚   â”œâ”€â”€ llama-embedding          # ExecutÃ¡vel compilado
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ nomic-embed-text-v1.5.Q4_K_M.gguf
â””â”€â”€ embed-kit/                 # Nossa biblioteca
    â””â”€â”€ node_modules/           # DependÃªncias do projeto
```
 
---
 
## ğŸš€ **Ready to Start!**
 
**PrÃ³ximo passo:** Implementar Task 1 - AnÃ¡lise completa da API llama.cpp