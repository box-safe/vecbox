# Native Llama.cpp Module

MÃ³dulo Node.js nativo para embeddings locais usando Llama.cpp diretamente.

## ðŸ”¨ Build

### PrÃ©-requisitos
- Node.js 16+
- Python 3.8+
- C++ compiler (GCC/Clang/MSVC)
- CMake 3.16+

### Build Manual
```bash
cd native
npm install
npm run build
```

### Build AutomÃ¡tico
```bash
# Do projeto raiz
npm run build:native
```

## ðŸ“¦ Estrutura

```
native/
â”œâ”€â”€ binding.gyp           <- ConfiguraÃ§Ã£o build
â”œâ”€â”€ llama_embedding.cpp  <- CÃ³digo C++ principal  
â”œâ”€â”€ index.js            <- Interface JS
â”œâ”€â”€ package.json        <- Deps especÃ­ficas
â”œâ”€â”€ build/Release/     <- BinÃ¡rio compilado
â””â”€â”€ README.md          <- Este arquivo
```

## ðŸš€ Uso

```javascript
const llama = require('./native');

// Carrega modelo
const model = llama.create('path/to/model.gguf');

// Gera embedding
const embedding = model.embed('Hello world');

// Libera recursos
model.close();
```

## ðŸ”§ IntegraÃ§Ã£o

O mÃ³dulo Ã© automaticamente importado pelo `LlamaCppProvider` com fallback para HTTP se nÃ£o disponÃ­vel.

## ðŸ› Troubleshooting

### Build falha
- Verifique se as dependÃªncias do sistema estÃ£o instaladas
- Certifique-se de que o Node.js versÃ£o 16+ estÃ¡ sendo usado
- Verifique se o CMake estÃ¡ disponÃ­vel

### MÃ³dulo nÃ£o carrega
- Verifique se o binÃ¡rio `llama_embedding.node` foi gerado
- Verifique se a arquitetura do binÃ¡rio corresponde ao sistema
- Consulte os logs para detalhes do erro
